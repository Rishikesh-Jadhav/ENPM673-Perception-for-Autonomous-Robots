# ENPM673-Perception-for-Autonomous-Robots
This repository serves as a record of my academic experience in ENPM673 during the Spring of 2023. It includes my solutions and code submissions for all projects. Each project is organized within its respective folder, complete with accompanying documentation and any necessary resources.

## ðŸ“š Course Overview
The Perception course offers an in-depth exploration of Classic Computer Vision principles and fundamental deep learning techniques. The curriculum centers on the augmentation of autonomous systems, encompassing robots, autonomous cars, and smart cameras. 
These hands-on projcts experiences helped us understand practical applications, ranging from lane detection for autonomous driving to the intricate task of constructing 3D models from 2D images. The course provides a holistic understanding of perception in autonomous systems, fostering both theoretical knowledge and practical skills.

1. **Curve Fitting and Trend Analysis:**
   - Identify the optimal trend line for a set of data points through curve fitting.

2. **Image Feature Recognition:**
   - Recognize key features in images, including corners, edges, and straight lines.

3. **3D Object Estimation:**
   - Estimate 3D information of objects based on their 2D images.

4. **Object Motion Metrics:**
   - Calculate motion metrics for objects, encompassing speed and direction, using camera feeds.

5. **Camera Pose Estimation:**
   - Conduct camera pose estimation for spatial understanding.

6. **Basic Image-based Machine Learning:**
   - Apply fundamental machine learning techniques to tasks involving images.

The course structure is enriched with four distinct projects, each described below.

## ðŸ“„ Project List
- Click [here](https://github.com/Rishikesh-Jadhav/VelocityEstimation-OpticalFlow) access ENPM-673 Final Project .

### [Project 1](https://github.com/Rishikesh-Jadhav/ENPM673-Perception-for-Autonomous-Robots/blob/main/project1/Project1.pdf): Rendering with Pytorch3D

- **Learnings from Project 1**:

  1. __Rendering First Mesh:__ Acquired a basic understanding of mesh rendering using PyTorch3D.

  2. __Practicing with Cameras:__ Created 360-degree gifs and set camera viewpoints for rendering.

  3. __Re-creating the Dolly Zoom:__ Successfully implemented the Dolly Zoom effect in PyTorch3D.

  4. __Practicing with Meshes:__ Created and rendered 3D shapes such as a tetrahedron and a cube.

  5. __Re-texturing a Mesh:__ Changed mesh colors based on vertex positions.

  6. __Camera Transformations:__  Implemented camera pose transformations for rendering.

  7. __Rendering Generic 3D Representations:__ Rendered point clouds and constructed them from RGB-D images. Parametrically generated and rendered point clouds.
   
  8. __Implicit Surfaces:__ Utilized implicit functions to define surfaces and converted them to meshes. Rendered a torus mesh with an implicit function and discussed mesh vs. point cloud rendering trade-offs.

- **[Project 1 Report](#)**    
  
### [Project 2](https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision/tree/main/Assignment2):  Single View to 3D

- **Learnings from Project 2**:
- **[Project 2 Report](#)**    
 
    
### [Project 3](https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision/tree/main/Assignment3):  Volume Rendering and Neural Radiance Fields

- **Learnings from Project 3**:
- **[Project 3 Report](#)**    


### [Project 4](https://github.com/Rishikesh-Jadhav/CMSC848F-3D-Vision/tree/main/Assignment4): Point Cloud Classification and Segmentation

- **Learnings from Project 4**:
- **[Project 4 Report](#)**    

  
## Additional Resources
- [Course related resources](https://academiccatalog.umd.edu/graduate/courses/enpm/)


